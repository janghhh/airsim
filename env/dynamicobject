# from MARL_test import ParallelEnv
import gym
import numpy as np
import airsim
import math
import time
import random


class AirSimMultiDroneEnv:
    metadata = {"render_modes": []}

    def __init__(
        self,
        ip_address="127.0.0.1",
        follower_names=("Follower0", "Follower1", "Follower2"),
        step_length=1.0,
        fixed_z=-10.0,
        leader_velocity=6.0,
        optimal_distance=10.0,
        far_cutoff=60.0,
        too_close=1.0,
        dt=0.05,
        do_visualize=True
    ):
        super().__init__()
        self.possible_agents = list(follower_names)
        self.agents = self.possible_agents[:]

        self.client = airsim.MultirotorClient(ip=ip_address)
        self.client.confirmConnection()
        self.dynamic_name = "DynamicObstacle"

        self.step_length = float(step_length)
        self.fixed_z = float(fixed_z)
        self.dt = float(dt)
        self.do_visualize = bool(do_visualize)
        self.leader_velocity = float(leader_velocity)
        self.optimal_distance = float(optimal_distance)
        self.far_cutoff = float(far_cutoff)
        self.too_close = float(too_close)
        self.follower_names = list(follower_names)
        self._first_setup = True

        self.dist_clip_max = max(self.far_cutoff, self.optimal_distance * 3.0)
        
        '''
        ê´€ì¸¡ê°’ì˜ ê°œìˆ˜  
        '''

        obs_dim = 3 + 36 
        act_dim = 2
        share_obs_dim = obs_dim * len(self.possible_agents)

        self.observation_spaces = {
            agent: gym.spaces.Box(
                low=np.array([0.0, -200.0, -200.0] + [0.0]*36, dtype=np.float32),
                high=np.array([self.dist_clip_max, 200.0, 200.0] + [self.far_cutoff]*36, dtype=np.float32),
                shape=(obs_dim,), dtype=np.float32
            )
            for agent in self.possible_agents
        }

        self.action_spaces = {
            agent: gym.spaces.Box(low=-1.0, high=1.0, shape=(act_dim,), dtype=np.float32)
            for agent in self.possible_agents
        }

        per_agent_low = [0.0, -200.0, -200.0] + [0.0]*36
        per_agent_high = [self.dist_clip_max, 200.0, 200.0] + [self.far_cutoff]*36
        self.share_observation_spaces = gym.spaces.Box(
            low=np.array(per_agent_low * len(self.possible_agents), dtype=np.float32),
            high=np.array(per_agent_high * len(self.possible_agents), dtype=np.float32),
            shape=(share_obs_dim,),
            dtype=np.float32
        )

        self.current_waypoint_idx = 0
        self._setup_flight()
        self._generate_leader_waypoints()
        self._last_visualize_t = time.time()

    '''
    ë¼ì´ë‹¤ ê´€ì¸¡ -> ì—¬ê¸°ë¥¼ ì˜ ìˆ˜ì •í•´ì•¼í•œë‹¤.
    ì´ˆê¸°í•¨ìˆ˜ì— lidar_names ì— ëŒ€í•œ ì´ˆê¸°í™”ë¥¼ í•´ì£¼ì–´ì•¼í•  ê²ƒ ê°™ë‹¤.
    '''
    
    def _get_lidar_obs(self, agent, lidar_name="LidarSensor1"):
        lidar_data = self.client.getLidarData(vehicle_name=agent, lidar_name=lidar_name)
        if len(lidar_data.point_cloud) < 3:
            return np.full(36, self.far_cutoff, dtype=np.float32)

        pts = np.array(lidar_data.point_cloud, dtype=np.float32).reshape(-1, 3)
        dists = np.linalg.norm(pts[:, :2], axis=1)
        angles = np.arctan2(pts[:, 1], pts[:, 0])

        bins = np.linspace(-math.pi, math.pi, 37)
        min_dists = np.full(36, self.far_cutoff, dtype=np.float32)

        for i in range(36):
            sel = dists[(angles >= bins[i]) & (angles < bins[i+1])]
            if len(sel) > 0:
                min_dists[i] = np.min(sel)
        return min_dists

    @property
    def observation_space(self):
        return [self.observation_spaces[a] for a in self.possible_agents]

    @property
    def action_space(self):
        return [self.action_spaces[a] for a in self.possible_agents]

    @property
    def share_observation_space(self):
        return [self.share_observation_spaces for _ in self.possible_agents]

    def seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)

    # --------------------- ì´ˆê¸°í™”/ì´ë¥™ ---------------------
    def _setup_flight(self):
        if self._first_setup:
            self.client.reset()
            # Drone1
            self.client.enableApiControl(True, "Drone1")
            self.client.armDisarm(True, "Drone1")

            # Followers
            for agent in self.possible_agents:
                self.client.enableApiControl(True, vehicle_name=agent)
                self.client.armDisarm(True, vehicle_name=agent)

            # <<< DynamicObstacle ì¶”ê°€ >>>
            self.client.enableApiControl(True, vehicle_name=self.dynamic_name)
            self.client.armDisarm(True, vehicle_name=self.dynamic_name)

            # ì´ë¥™
            futs = [self.client.takeoffAsync(vehicle_name="Drone1")]
            futs += [self.client.takeoffAsync(vehicle_name=a) for a in self.possible_agents]
            futs += [self.client.takeoffAsync(vehicle_name=self.dynamic_name)]   # ì¶”ê°€
            for f in futs: f.join()

            time.sleep(1.0)
            self._teleport_to_start()
            self._first_setup = False

    # --------------------- ìœ ì¸ê¸° ê²½ë¡œ ìƒì„± ---------------------
    def _generate_leader_waypoints(self):
        leader_start_pos = np.array([5.0, 2.5, self.fixed_z])
        distance = random.uniform(100.0, 100.0)
        angle = random.uniform(0, 2 * np.pi)
        final_destination = leader_start_pos + np.array([
            distance * np.cos(angle),
            distance * np.sin(angle),
            0
        ])
        self.leader_waypoints = [final_destination]
        self.current_waypoint_idx = 0

        try:
            object_name = "target1v1_5"
            flag_position = airsim.Vector3r(float(final_destination[0]),
                                           float(final_destination[1]),
                                           float(self.fixed_z + 8.0))
            flag_orientation = airsim.to_quaternion(0, 80.1, 0)
            flag_pose = airsim.Pose(flag_position, flag_orientation)
            self.client.simSetObjectPose(object_name, flag_pose)
        except Exception as e:
            print(f"'{object_name}' ê°ì²´ë¥¼ ì´ë™ ì‹¤íŒ¨. ì–¸ë¦¬ì–¼ ë ˆë²¨ì— í•´ë‹¹ ì´ë¦„ì˜ ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸.")
            print(e)

    # --------------------- ìœ ì¸ê¸° ì´ë™ ---------------------
    def _update_leader_movement(self):
        if not self.leader_waypoints:
            self._generate_leader_waypoints()
        target = self.leader_waypoints[0]
        pose = self.client.simGetVehiclePose(vehicle_name="Drone1")
        cur = np.array([pose.position.x_val, pose.position.y_val, pose.position.z_val])
        dist_to_target = np.linalg.norm(target[:2] - cur[:2])
        if dist_to_target < 3.0:
            return True

        dir_vec = target - cur
        dist = np.linalg.norm(dir_vec[:2])
        if dist > 1e-6:
            dir_unit = dir_vec / (dist + 1e-9)
            move = dir_unit * self.leader_velocity * self.dt
            new_pos = cur + move
            self.client.simSetVehiclePose(
                airsim.Pose(airsim.Vector3r(new_pos[0], new_pos[1], self.fixed_z),
                            airsim.Quaternionr()),
                ignore_collision=True,
                vehicle_name="Drone1"
            )

        if self.do_visualize:
            now = time.time()
            if (now - self._last_visualize_t) >= 0.1:
                self.client.simFlushPersistentMarkers()
                self._visualize_circles()
                self._last_visualize_t = now

        return False

    # --------------------- ì‹œê°í™” ---------------------
    def _visualize_circles(self):
        try:
            leader_pos = self.client.simGetObjectPose("Drone1").position
            center = np.array([leader_pos.x_val, leader_pos.y_val, leader_pos.z_val], dtype=float)

            def ring_points(radius, n=36):
                pts = []
                for i in range(n+1):
                    ang = (i / n) * 2 * np.pi
                    x = center[0] + radius * np.cos(ang)
                    y = center[1] + radius * np.sin(ang)
                    z = center[2]
                    pts.append(airsim.Vector3r(x, y, z))
                return pts

            line_thickness = 20.0
            self.client.simPlotLineStrip(ring_points(self.optimal_distance), [1, 1, 0, 0.8], line_thickness, 0.15, True)
            self.client.simPlotLineStrip(ring_points(self.far_cutoff), [0, 1, 0, 0.8], line_thickness, 0.15, True)
        except Exception:
            pass

    # --------------------- ê´€ì¸¡ ---------------------
    def _get_obs(self, agent):
        leader_pos = self.client.simGetObjectPose("Drone1").position
        agent_pos = self.client.simGetObjectPose(agent).position
        rel = np.array([leader_pos.x_val - agent_pos.x_val,
                        leader_pos.y_val - agent_pos.y_val], dtype=np.float32)
        dist = float(np.linalg.norm(rel))
        clipped_dist = float(np.clip(dist, 0.0, self.dist_clip_max))
        base_obs = np.array([clipped_dist, rel[0], rel[1]], dtype=np.float32)

        if agent.startswith("Follower"):
            lidar_obs = self._get_lidar_obs(agent)
            obs = np.concatenate([base_obs, lidar_obs], axis=0).astype(np.float32)
        else:
            obs = base_obs
        return obs

    # --------------------- ì—ì´ì „íŠ¸ ì´ë™ ---------------------
    def _do_action(self, agent, action):
        a = np.clip(np.asarray(action, dtype=np.float32), -1.0, 1.0)
        pose = self.client.simGetVehiclePose(agent)
        x, y, z = pose.position.x_val, pose.position.y_val, self.fixed_z
        dx = float(a[0]) * float(self.step_length)
        dy = float(a[1]) * float(self.step_length)
        nx = float(x) + dx
        ny = float(y) + dy
        nz = float(z)
        new_pos = airsim.Vector3r(nx, ny, nz)
        new_pose = airsim.Pose(new_pos, airsim.Quaternionr(0.0, 0.0, 0.0, 1.0))
        self.client.simSetVehiclePose(new_pose, False, vehicle_name=agent)

    # --------------------- ë³´ìƒ ---------------------
    def _compute_reward(self, agent, obs=None):
        rewards_dict = {}
        agent_pos = self.client.simGetObjectPose(agent).position

        # ğŸ’¥ 1ï¸âƒ£ ë“œë¡  ê°„ ì¶©ëŒ íŒ¨ë„í‹° (ê¸°ì¡´)
        for other in self.agents:
            if other == agent:
                continue
            other_pos = self.client.simGetObjectPose(other).position
            dist_agents = np.linalg.norm([
                agent_pos.x_val - other_pos.x_val,
                agent_pos.y_val - other_pos.y_val
            ])
            if dist_agents < 0.5:
                rewards_dict['penalty_agent_collision'] = -1000.0
                return rewards_dict, True

        # ğŸ“¡ 2ï¸âƒ£ ë¼ì´ë‹¤ ê·¼ì ‘ ê²½ê³  (ê¸°ì¡´)
        lidar_obs = self._get_lidar_obs(agent)
        min_dist = np.min(lidar_obs)
        if min_dist < 1.0:
            rewards_dict['penalty_lidar_near'] = -500.0
            return rewards_dict, False

        # ğŸ‘ 3ï¸âƒ£ ê¸°ë³¸ ê±°ë¦¬ ê³„ì‚°
        if obs is None:
            obs = self._get_obs(agent)
        dist_raw = math.hypot(float(obs[1]), float(obs[2]))

        if dist_raw < self.too_close:
            rewards_dict['penalty_too_close'] = -1000.0
            return rewards_dict, True

        if dist_raw > self.far_cutoff:
            rewards_dict['penalty_too_far'] = -1000.0
            return rewards_dict, True

        # ğŸ§  4ï¸âƒ£ ë³´í˜¸ í–‰ë™ìš© ë³´ìƒ í•­ëª© ì¶”ê°€ -------------------------
        leader_pos = self.client.simGetObjectPose("Drone1").position
        obstacle_pos = self.client.simGetObjectPose(self.dynamic_name).position

        # ê±°ë¦¬ ê³„ì‚°
        dist_leader_obstacle = math.hypot(
            leader_pos.x_val - obstacle_pos.x_val,
            leader_pos.y_val - obstacle_pos.y_val
        )
        dist_agent_obstacle = math.hypot(
            agent_pos.x_val - obstacle_pos.x_val,
            agent_pos.y_val - obstacle_pos.y_val
        )

        # --- ë³´í˜¸ ê´€ë ¨ ë³´ìƒ ë¡œì§ ---
        shield_reward = 0.0

        # (1) ì¥ì• ë¬¼ì´ ìœ ì¸ê¸° ê·¼ì²˜ë¡œ ì ‘ê·¼í•˜ë©´ â†’ ìœ„í—˜ (ë³´í˜¸ ì‹¤íŒ¨ ê²½í–¥)
        if dist_leader_obstacle < 5.0:
            shield_reward -= 500.0

        # (2) ë°˜ëŒ€ë¡œ, íŒ”ë¡œì›Œê°€ ì¥ì• ë¬¼ì— ê·¼ì ‘í•´ ë°©ì–´í•˜ë©´ â†’ ë°©íŒ¨ ì„±ê³µ
        if dist_agent_obstacle < 3.0 and dist_leader_obstacle < 6.0:
            shield_reward += 1000.0  # ëŒ€ì‹  ë§‰ì•˜ë‹¤ê³  ê°„ì£¼

        # (3) ì¥ì• ë¬¼ê³¼ ìœ ì¸ê¸° ì‚¬ì´ë¥¼ ê°€ë¡œë§‰ëŠ” ìœ„ì¹˜ì— ìˆìœ¼ë©´ â†’ ì¶”ê°€ ë³´ìƒ
        vec_leader_obst = np.array([
            obstacle_pos.x_val - leader_pos.x_val,
            obstacle_pos.y_val - leader_pos.y_val
        ])
        vec_leader_agent = np.array([
            agent_pos.x_val - leader_pos.x_val,
            agent_pos.y_val - leader_pos.y_val
        ])
        dot = np.dot(vec_leader_obst, vec_leader_agent)
        if dot > 0:  # ê°™ì€ ë°©í–¥ì— ìˆì„ ë•Œ (ì¦‰, ë°©ì–´ì„  ìƒì— ìœ„ì¹˜)
            shield_reward += 300.0

        rewards_dict["shield_reward"] = shield_reward

        # ğŸ¯ 5ï¸âƒ£ ê¸°ë³¸ ê±°ë¦¬ ìœ ì§€ ë³´ìƒ (ê¸°ì¡´)
        sigma = 5.0
        if dist_raw <= self.optimal_distance:
            gauss_val = math.exp(-((dist_raw - self.optimal_distance) ** 2) / (2 * sigma**2))
            gauss_at_0 = math.exp(-(self.optimal_distance**2) / (2 * sigma**2))
            reward_val = 10.0 * (gauss_val - gauss_at_0) / (1 - gauss_at_0 + 1e-9)
        else:
            t = (dist_raw - self.optimal_distance) / (self.far_cutoff - self.optimal_distance + 1e-9)
            alpha = 6.0
            reward_val = 10.0 + (-1010.0) * ((math.exp(alpha * t) - 1.0) / (math.exp(alpha) - 1.0))

        rewards_dict['distance_reward'] = reward_val

        # ì „ì²´ ë³´ìƒ í•©ì‚°
        total_reward = shield_reward + reward_val
        rewards_dict["total_reward"] = total_reward

        return rewards_dict, False



    # --------------------- íŒ€ ë³´ìƒ ì§‘ê³„ ---------------------
    def _team_reward_and_done(self, per_agent_results, mission_accomplished=False):
        any_fail = any(done_i for (_, done_i) in per_agent_results)
        if any_fail:
            return -200.0, True, {"final_status": "FAIL_CRASH"}

        if mission_accomplished:
            tracking_rewards = [r_i for (r_i, _) in per_agent_results]
            final_reward = np.mean(tracking_rewards) + 500.0
            return final_reward, True, {"final_status": "SUCCESS"}

        tracking_rewards = [r_i for (r_i, _) in per_agent_results]
        return np.mean(tracking_rewards), False, {}
    


        # --------------------- ì´ˆê¸° ìœ„ì¹˜ ì„¤ì • ---------------------
    def _teleport_to_start(self):
        start_cfg = {
            "Drone1":    (5.0,  2.5, float(self.fixed_z)),
            "Follower0": (0.0,  0.0, float(self.fixed_z)),
            "Follower1": (0.0,  2.5, float(self.fixed_z)),
            "Follower2": (0.0,  5.0, float(self.fixed_z)),
            # â˜… ì¶”ê°€: ë™ì  ì¥ì• ë¬¼ ì‹œì‘ ìœ„ì¹˜ (ìœ ì¸ê¸° ê·¼ì²˜)
            self.dynamic_name: (5.0 + 10  , 2.5 + 10, float(self.fixed_z)),
        }

        # API ì œì–´ í™œì„±í™”
        self.client.enableApiControl(True, vehicle_name="Drone1")
        for agent in self.possible_agents:
            self.client.enableApiControl(True, vehicle_name=agent)
        self.client.enableApiControl(True, vehicle_name=self.dynamic_name)

        # ê° ë“œë¡  ìœ„ì¹˜ ì„¤ì •
        for name, (x, y, z) in start_cfg.items():
            self.client.simSetVehiclePose(
                airsim.Pose(airsim.Vector3r(float(x), float(y), float(z)),
                            airsim.Quaternionr(0.0, 0.0, 0.0, 1.0)),
                ignore_collision=True,
                vehicle_name=name
            )
        time.sleep(0.05)


    # --------------------- ë™ì ì¥ì• ë¬¼ FSM + ì§ì„ /ê³¡ì„  ì¶”ê²© ---------------------
    def _update_dynamic_obstacle(self, t):
        name = self.dynamic_name
        fixed_z = self.fixed_z
        attack_speed = 6.0
        STOP_DISTANCE = 1.0

        # ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        if not hasattr(self, "_obstacle_state"):
            self._obstacle_state = "IDLE"
            self._next_chase_time = time.time() + random.uniform(1.0, 3.0)
            self._idle_pos = None
            self._chase_mode = None  # 'STRAIGHT' or 'CURVED'

        leader_pose = self.client.simGetObjectPose("Drone1").position
        obstacle_pose = self.client.simGetObjectPose(name).position
        lx, ly, lz = leader_pose.x_val, leader_pose.y_val, leader_pose.z_val
        cx, cy, cz = obstacle_pose.x_val, obstacle_pose.y_val, obstacle_pose.z_val

        dx, dy = lx - cx, ly - cy
        dist_2d = math.sqrt(dx**2 + dy**2) + 1e-9

        # ------------------ IDLE ëª¨ë“œ ------------------
        if self._obstacle_state == "IDLE":
            if self._idle_pos is None:
                radius = random.uniform(30.0, 50.0)
                angle = random.uniform(0, 2 * np.pi)
                self._idle_pos = (lx + radius * np.cos(angle), ly + radius * np.sin(angle))
                print(f"[ğŸŸ¢ ëŒ€ê¸°ëª¨ë“œ] {radius:.1f}m ê±°ë¦¬ì—ì„œ ëŒ€ê¸°")

            ix, iy = self._idle_pos
            dx_i, dy_i = ix - cx, iy - cy
            dist_idle = math.sqrt(dx_i**2 + dy_i**2)
            if dist_idle > 1.0:
                vx = dx_i / dist_idle * 2.0
                vy = dy_i / dist_idle * 2.0
                vz = (fixed_z - cz) * 0.3
                self.client.moveByVelocityAsync(vx, vy, vz, duration=0.1, vehicle_name=name) # duration=0.2
            else:
                self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=name)

            # ì¼ì • ì‹œê°„ í›„ ì¶”ê²© ì‹œì‘
            if time.time() > self._next_chase_time:
                self._obstacle_state = "CHASE"
                self._chase_start = time.time()
                self._chase_mode = random.choice(["STRAIGHT", "CURVED"])
                print(f"[ğŸš€ ì¶”ê²© ì‹œì‘] ëª¨ë“œ: {self._chase_mode}")
                return

        # ------------------ CHASE ëª¨ë“œ ------------------
        elif self._obstacle_state == "CHASE":
            elapsed = time.time() - self._chase_start
            chase_duration = random.uniform(10.0, 11.0)

            if elapsed > chase_duration:
                print(f"[âšª ì¶”ê²© ì¢…ë£Œ] ({self._chase_mode}) {elapsed:.1f}s í›„ ëŒ€ê¸° ë³µê·€")
                self._obstacle_state = "RETURN"
                self._idle_pos = None
                self._next_chase_time = time.time() + random.uniform(1.0, 3.0)
                return

            if dist_2d <= STOP_DISTANCE:
                # print(f"[âœ… ì •ì§€] Drone1ê³¼ ê±°ì˜ ì¼ì¹˜ (ê±°ë¦¬={dist_2d:.2f}m)")
                self.client.moveByVelocityAsync(0, 0, 0, duration=0.1, vehicle_name=name)
                return

            dir_x = dx / dist_2d
            dir_y = dy / dist_2d
            vz = np.clip((fixed_z - cz) * 3.0, -2.0, 2.0)

            # --- (1) ì§ì„  ì¶”ê²© ---
            if self._chase_mode == "STRAIGHT":
                vx = dir_x * attack_speed
                vy = dir_y * attack_speed

            # --- (2) ê³¡ì„  ì¶”ê²© ---
            else:
                amplitude = 10.0
                freq = 1.0
                phase = math.sin(2.0 * math.pi * freq * t/2)
                perp_x = -dir_y
                perp_y = dir_x
                vx = (dir_x * attack_speed) + (perp_x * amplitude * phase)
                vy = (dir_y * attack_speed) + (perp_y * amplitude * phase)

                self.client.moveByVelocityZAsync(float(vx), float(vy), float(self.fixed_z), 0.1,vehicle_name=name)


            self.client.moveByVelocityAsync(vx, vy, vz, duration=0.1, vehicle_name=name)

        # ------------------ RETURN ëª¨ë“œ ------------------
        elif self._obstacle_state == "RETURN":
            if self._idle_pos is None:
                radius = random.uniform(30.0, 50.0)
                angle = random.uniform(0, 2 * np.pi)
                self._idle_pos = (lx + radius * np.cos(angle), ly + radius * np.sin(angle))
                print(f"[ğŸ”„ ëŒ€ê¸° ìœ„ì¹˜ ì¬ì„¤ì •] ìƒˆ ì§€ì ìœ¼ë¡œ ë³µê·€ ì¤‘")

            ix, iy = self._idle_pos
            dx_r, dy_r = ix - cx, iy - cy
            dist_return = math.sqrt(dx_r**2 + dy_r**2)
            if dist_return > 1.0:
                vx = dx_r / dist_return * 3.0
                vy = dy_r / dist_return * 3.0
                vz = (fixed_z - cz) * 0.3
                self.client.moveByVelocityAsync(vx, vy, vz, duration=0.1, vehicle_name=name)
            else:
                self._obstacle_state = "IDLE"
                self._next_chase_time = time.time() + random.uniform(1.0, 3.0)
                print("[ğŸŸ¢ ëŒ€ê¸°ëª¨ë“œ ë³µê·€ ì™„ë£Œ]")








    # --------------------- PettingZoo API ---------------------
    def reset(self, seed=None, options=None):
        self.agents = self.possible_agents[:]
        self._setup_flight()
        self._generate_leader_waypoints()
        self.current_waypoint_idx = 0
        self._teleport_to_start()
        self.client.simFlushPersistentMarkers()
         # ğŸ”§ ë™ì ì¥ì• ë¬¼ ìƒíƒœ ì´ˆê¸°í™” (ì¤‘ìš”)
        self._obstacle_state = "IDLE"
        self._idle_pos = None
        self._next_chase_time = time.time() + random.uniform(1.0, 3.0)
        self._chase_mode = None
        print("[â™»ï¸ Reset] ë™ì ì¥ì• ë¬¼ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")

        return [self._get_obs(a) for a in self.agents]

    def step(self, actions):
        t = time.time()  # í˜„ì¬ ì‹œê°„ (ë˜ëŠ” self.step_count ë“±)
        self._update_dynamic_obstacle(t)  # ì¥ì• ë¬¼ ê°±ì‹ 

        for agent, act in zip(self.agents, actions):
            self._do_action(agent, act)
        mission_accomplished = self._update_leader_movement()

         # --------------------- ğŸ’¥ ìœ ì¸ê¸°-ë™ì ì¥ì• ë¬¼ ì¶©ëŒ ê°ì§€ ---------------------
        leader_pos = self.client.simGetObjectPose("Drone1").position
        obstacle_pos = self.client.simGetObjectPose(self.dynamic_name).position

        dx = leader_pos.x_val - obstacle_pos.x_val
        dy = leader_pos.y_val - obstacle_pos.y_val
        dist_leader_obstacle = math.sqrt(dx**2 + dy**2)

        if dist_leader_obstacle <= 0.5:  # ì¶©ëŒ ì„ê³„ê°’ (m)
            print(f"[ğŸ’¥ ì¶©ëŒ ê°ì§€] Drone1 â†” {self.dynamic_name} ê±°ë¦¬ = {dist_leader_obstacle:.2f}m â†’ ì—í”¼ì†Œë“œ ì¢…ë£Œ")

            # ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ë™ì¼í•œ í˜ë„í‹°ì™€ ì¢…ë£Œ ìƒíƒœë¥¼ ë°›ìŒ
            obs_list = [self._get_obs(a) for a in self.agents]
            rewards_list = [-1000.0] * len(self.agents)
            dones_list = [True] * len(self.agents)
            infos_list = [
                {"collision": "leader_obstacle", "final_status": "FAIL_COLLISION"}
                for _ in self.agents
            ]

            return obs_list, rewards_list, dones_list, infos_list
        
        # # --------------------- â­ï¸ ì¶”ê°€: ì—ì´ì „íŠ¸-ìœ ì¸ê¸° ê±°ë¦¬ ì¶œë ¥ â­ï¸ ---------------------
        # print("[íŒ”ë¡œì›Œ-ìœ ì¸ê¸° ê±°ë¦¬]")
        # for agent in self.agents:
        #     # _get_obs í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ë¦¬(dist) ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        #     # obs[0]ì€ clipped_dist, obs[1]ì€ rel_x, obs[2]ëŠ” rel_y ì…ë‹ˆë‹¤.
        #     o = self._get_obs(agent)
            
        #     # obs[1]ê³¼ obs[2]ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœìˆ˜í•œ X-Y í‰ë©´ ê±°ë¦¬(dist_raw)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        #     rel_x = float(o[1])
        #     rel_y = float(o[2])
        #     dist_raw = math.hypot(rel_x, rel_y)
            
        #     print(f"  > {agent}: {dist_raw:.2f}m")
        # # ---------------------------------------------------------------------

        obs_list = []
        per_agent_results = []
        per_agent_detailed_infos = []

        for agent in self.agents:
            o = self._get_obs(agent)
            rewards_dict, done_i = self._compute_reward(agent, obs=o)
            total_reward = sum(rewards_dict.values())
            obs_list.append(o)
            per_agent_results.append((total_reward, done_i))
            per_agent_detailed_infos.append({"rewards": rewards_dict})

        team_reward, done_all, final_team_info = self._team_reward_and_done(per_agent_results, mission_accomplished)

        n = len(self.agents)
        rewards_list = [team_reward] * n
        dones_list = [done_all] * n
        infos_list = []

        for i in range(n):
            agent_info = per_agent_detailed_infos[i].copy()
            agent_info.update(final_team_info)
            infos_list.append(agent_info)

        return obs_list, rewards_list, dones_list, infos_list
